# HTML Reports

HTML reports in Perfornium provide comprehensive, interactive visualizations of your performance test results. These reports include charts, statistics, and detailed analysis that can be shared with stakeholders and used for performance monitoring.

## Basic Configuration

### Simple HTML Report

```yaml
report:
  generate: true
  output: "reports/test-report.html"
```

### Timestamped HTML Report

```yaml
report:
  generate: true
  output: "reports/test-report-{{timestamp}}.html"
```

### Custom Report Configuration

```yaml
report:
  generate: true
  output: "reports/comprehensive-report-{{timestamp}}.html"
  template: "detailed"              # default, minimal, detailed, custom
  title: "API Performance Test Results"
  description: "Load test results for the user authentication API"
  include_raw_data: false           # Include raw request/response data
  auto_refresh: false               # Auto-refresh for real-time reports
  refresh_interval: 30              # Refresh every 30 seconds (if auto_refresh)
```

## Report Templates

### Default Template

The default template provides a balanced view with essential metrics and charts:

```yaml
report:
  generate: true
  output: "reports/default-report.html"
  template: "default"
  sections:
    - "summary"
    - "response_time_charts" 
    - "throughput_charts"
    - "error_analysis"
    - "scenario_breakdown"
```

**Includes:**
- Executive Summary
- Response Time Timeline
- Response Time Distribution
- Throughput Over Time
- Error Rate Analysis
- Scenario Performance Breakdown

### Minimal Template

Lightweight report with key metrics only:

```yaml
report:
  generate: true
  output: "reports/minimal-report.html"
  template: "minimal"
  sections:
    - "summary"
    - "key_metrics"
```

**Includes:**
- Test Summary
- Key Performance Indicators
- Pass/Fail Status

### Detailed Template

Comprehensive report with all available metrics and charts:

```yaml
report:
  generate: true
  output: "reports/detailed-report.html"
  template: "detailed"
  sections:
    - "executive_summary"
    - "test_configuration"
    - "load_pattern_analysis"
    - "response_time_analysis"
    - "throughput_analysis"
    - "error_analysis"
    - "scenario_breakdown"
    - "step_performance"
    - "system_resources"
    - "sla_compliance"
    - "recommendations"
    - "raw_data_sample"
```

## Custom Report Configuration

### Section Configuration

```yaml
report:
  generate: true
  output: "reports/custom-report.html"
  template: "custom"
  
  # Executive summary configuration
  executive_summary:
    show_test_info: true
    show_duration: true
    show_load_pattern: true
    show_key_metrics: true
    sla_thresholds:
      p95_response_time: 2000
      error_rate: 0.01
      throughput: 100
    
  # Chart configurations  
  charts:
    response_time_timeline:
      enabled: true
      chart_type: "line"            # line, area, bar
      time_window: "all"            # all, last_hour, custom
      aggregation: "1m"             # Time bucket size
      percentiles: [50, 95, 99]
      show_error_rate: true
      
    response_time_distribution:
      enabled: true
      chart_type: "histogram"       # histogram, box_plot
      bins: 50
      show_percentiles: true
      log_scale: false
      
    throughput_chart:
      enabled: true
      chart_type: "area"
      time_window: "all"
      aggregation: "30s"
      show_target_line: true
      target_throughput: 200
      
    error_analysis:
      enabled: true
      chart_type: "pie"             # pie, bar, table
      group_by: "error_type"        # error_type, status_code, scenario
      show_top: 10
      
  # Table configurations
  tables:
    scenario_summary:
      enabled: true
      columns:
        - "scenario"
        - "requests"
        - "avg_response_time"
        - "p95_response_time"
        - "success_rate"
        - "throughput"
      sort_by: "p95_response_time"
      sort_order: "desc"
      
    step_performance:
      enabled: true
      show_all_steps: false
      show_top_slowest: 20
      show_top_errors: 10
```

### Custom Styling

```yaml
report:
  generate: true
  output: "reports/branded-report.html"
  styling:
    theme: "custom"
    primary_color: "#1f77b4"
    secondary_color: "#ff7f0e" 
    background_color: "#ffffff"
    text_color: "#333333"
    font_family: "Arial, sans-serif"
    logo_url: "https://company.com/logo.png"
    css_file: "custom-styles.css"
    
  branding:
    company_name: "Acme Corp"
    report_title: "Production API Load Test"
    footer_text: "Generated by Perfornium Performance Testing Framework"
    contact_email: "performance-team@company.com"
```

## Interactive Features

### Real-time Reports

Generate reports that update during test execution:

```yaml
report:
  generate: true
  output: "reports/live-report-{{timestamp}}.html"
  real_time: true
  refresh_interval: 15            # Update every 15 seconds
  auto_refresh: true              # Auto-refresh in browser
  websocket_enabled: true         # Real-time updates via WebSocket
  live_charts:
    - "response_time_timeline"
    - "throughput_chart"
    - "error_rate"
    - "active_users"
```

### Drill-Down Analysis

Enable interactive drilling into data:

```yaml
report:
  generate: true
  output: "reports/interactive-report.html"
  interactive:
    enabled: true
    drill_down:
      scenario_to_steps: true     # Click scenario to see steps
      time_range_zoom: true       # Zoom into time ranges
      filter_by_status: true      # Filter by success/failure
      error_details: true         # Click errors for details
    data_export:
      csv_export: true            # Export data to CSV
      json_export: true           # Export data to JSON
      image_export: true          # Export charts as images
```

### Comparison Reports

Compare multiple test runs:

```yaml
report:
  generate: true
  output: "reports/comparison-report.html"
  template: "comparison"
  comparison:
    baseline: "results/baseline-test.json"
    current: "results/current-test.json"
    previous_runs:
      - "results/test-run-1.json"
      - "results/test-run-2.json"
    metrics_to_compare:
      - "p95_response_time"
      - "throughput"
      - "error_rate"
      - "success_rate"
    change_thresholds:
      improvement: 0.05           # 5% improvement
      regression: 0.10            # 10% regression
```

## Advanced Metrics

### Performance Percentiles

```yaml
report:
  generate: true
  output: "reports/percentile-analysis.html"
  metrics:
    response_time_percentiles:
      enabled: true
      percentiles: [50, 75, 90, 95, 99, 99.9]
      show_histogram: true
      show_box_plots: true
      
    throughput_percentiles:
      enabled: true
      time_window: "1m"
      percentiles: [25, 50, 75, 95]
      
    error_rate_percentiles:
      enabled: true
      percentiles: [90, 95, 99]
```

### Load Pattern Analysis

```yaml
report:
  generate: true
  output: "reports/load-pattern-analysis.html"
  load_analysis:
    enabled: true
    show_vu_rampup: true
    show_load_phases: true
    correlate_performance: true   # Correlate performance with load
    identify_bottlenecks: true    # Highlight bottleneck points
    capacity_analysis: true       # Analyze capacity limits
```

### SLA Compliance Reporting

```yaml
report:
  generate: true
  output: "reports/sla-compliance.html"
  sla_reporting:
    enabled: true
    sla_definitions:
      availability:
        threshold: 99.9           # 99.9% uptime
        measurement: "success_rate"
      response_time:
        threshold: 2000           # 2 second P95
        measurement: "p95_response_time"
      throughput:
        threshold: 100            # 100 RPS minimum
        measurement: "min_throughput"
    compliance_scoring: true
    breach_analysis: true
    recommendations: true
```

## Data Integration

### External Data Sources

```yaml
report:
  generate: true
  output: "reports/integrated-report.html"
  data_sources:
    # System metrics from monitoring tools
    system_metrics:
      source: "prometheus"
      url: "http://prometheus:9090"
      queries:
        cpu_usage: "avg(cpu_usage_percent)"
        memory_usage: "avg(memory_usage_percent)"
        disk_io: "rate(disk_io_bytes[5m])"
        
    # Application metrics
    app_metrics:
      source: "influxdb"
      url: "http://influxdb:8086"
      database: "application_metrics"
      queries:
        db_connections: "SELECT mean(connections) FROM database"
        cache_hit_rate: "SELECT mean(hit_rate) FROM cache"
        
    # Business metrics
    business_metrics:
      source: "custom_api"
      url: "http://business-metrics.company.com/api"
      endpoints:
        revenue_impact: "/revenue/during-test"
        user_satisfaction: "/satisfaction/during-test"
```

### Historical Trending

```yaml
report:
  generate: true
  output: "reports/trending-report.html"
  historical_analysis:
    enabled: true
    data_retention: "30d"         # Keep 30 days of history
    baseline_calculation: "7d_avg" # 7-day average baseline
    trend_analysis:
      response_time: true
      throughput: true
      error_rate: true
      resource_utilization: true
    anomaly_detection:
      enabled: true
      sensitivity: "medium"       # low, medium, high
      alert_thresholds:
        response_time_increase: 1.5  # 50% increase
        throughput_decrease: 0.8     # 20% decrease
        error_rate_increase: 2.0     # 100% increase
```

## Report Delivery

### Automated Distribution

```yaml
report:
  generate: true
  output: "reports/distributed-report-{{timestamp}}.html"
  distribution:
    enabled: true
    recipients:
      - email: "dev-team@company.com"
        role: "development"
        sections: ["summary", "error_analysis"]
      - email: "ops-team@company.com"  
        role: "operations"
        sections: ["system_resources", "sla_compliance"]
      - email: "management@company.com"
        role: "management"
        sections: ["executive_summary", "recommendations"]
    delivery_methods:
      - type: "email"
        smtp_server: "smtp.company.com"
        subject: "Performance Test Results - {{test_name}}"
        body_template: "email-template.html"
      - type: "slack"
        webhook_url: "https://hooks.slack.com/webhook"
        channel: "#performance-alerts"
        message_template: "slack-template.json"
      - type: "s3"
        bucket: "performance-reports"
        path: "reports/{{timestamp}}"
        public_url: true
```

### Report Archival

```yaml
report:
  generate: true
  output: "reports/archived-report-{{timestamp}}.html"
  archival:
    enabled: true
    retention_policy: "90d"       # Keep reports for 90 days
    compression: "gzip"           # Compress old reports
    storage_backends:
      - type: "local"
        path: "archive/reports"
      - type: "s3"
        bucket: "performance-archive"
        storage_class: "STANDARD_IA"
      - type: "azure_blob"
        container: "reports"
        tier: "cool"
```

## Performance and Optimization

### Large Dataset Handling

```yaml
report:
  generate: true
  output: "reports/large-dataset-report.html"
  performance:
    max_data_points: 100000       # Limit data points in charts
    sampling_strategy: "uniform"   # uniform, time_based, smart
    chart_optimization: true       # Optimize charts for performance
    lazy_loading: true            # Load sections on demand
    data_compression: true        # Compress embedded data
    memory_limit: "512MB"         # Memory limit for report generation
```

### Generation Optimization

```yaml
report:
  generate: true
  output: "reports/optimized-report.html"
  generation:
    parallel_processing: true     # Generate sections in parallel
    cache_enabled: true          # Cache intermediate results
    incremental_updates: true    # Update only changed sections
    template_caching: true       # Cache report templates
    asset_minification: true     # Minify CSS/JS assets
```

## Best Practices

### 1. Choose Appropriate Template

```yaml
# For executive stakeholders
report:
  template: "minimal"
  sections: ["executive_summary", "sla_compliance"]

# For technical teams  
report:
  template: "detailed"
  sections: ["response_time_analysis", "error_analysis", "system_resources"]

# For performance monitoring
report:
  template: "default"
  real_time: true
  auto_refresh: true
```

### 2. Configure Meaningful Thresholds

```yaml
report:
  sla_thresholds:
    p95_response_time: 2000     # Based on user expectations
    error_rate: 0.01            # Based on business requirements
    throughput: 100             # Based on capacity planning
```

### 3. Enable Interactive Features

```yaml
report:
  interactive:
    enabled: true
    drill_down: true
    data_export: true
  # Allows stakeholders to explore data themselves
```

### 4. Plan for Large Tests

```yaml
report:
  performance:
    max_data_points: 50000
    sampling_strategy: "smart"
    lazy_loading: true
  # Ensures reports load quickly even with large datasets
```

### 5. Customize for Your Audience

```yaml
report:
  branding:
    company_name: "Your Company"
    logo_url: "company-logo.png"
  sections:
    # Include sections relevant to your stakeholders
```

HTML reports provide a powerful way to visualize and share performance test results, making complex performance data accessible to both technical and non-technical stakeholders.